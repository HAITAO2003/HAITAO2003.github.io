<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exact Solution Methods - Value Iteration and Policy Iteration - Haitao Wang</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css">
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        .blog-post-content {
            max-width: 800px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .blog-post-content h1 {
            font-size: 2.5rem;
            color: var(--primary-color);
            margin-bottom: 20px;
        }

        .blog-post-content h2 {
            font-size: 1.8rem;
            color: var(--primary-color);
            margin-top: 40px;
            margin-bottom: 20px;
        }

        .blog-post-content h3 {
            font-size: 1.4rem;
            color: var(--text-dark);
            margin-top: 30px;
            margin-bottom: 15px;
        }

        .blog-post-content p {
            margin-bottom: 20px;
            line-height: 1.8;
            color: var(--text-dark);
        }

        .blog-post-content ul {
            margin: 20px 0;
            padding-left: 30px;
        }

        .blog-post-content li {
            margin-bottom: 10px;
            line-height: 1.8;
        }

        .blog-post-content ol {
            margin: 20px 0;
            padding-left: 30px;
        }

        .blog-post-content ol li {
            margin-bottom: 15px;
            line-height: 1.8;
        }

        .blog-figure {
            margin: 30px 0;
            text-align: center;
        }

        .blog-figure img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: var(--shadow-md);
        }

        .blog-figure figcaption {
            margin-top: 10px;
            font-style: italic;
            color: var(--text-light);
        }

        .algorithm-box {
            background: var(--light-bg);
            border-left: 4px solid var(--accent-color);
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .math-block {
            margin: 15px 0;
            overflow-x: auto;
            text-align: left;
        }

        .iteration-images {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .iteration-image {
            text-align: center;
        }

        .iteration-image img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: var(--shadow-md);
            margin-bottom: 10px;
        }

        .iteration-image p {
            font-size: 0.9rem;
            color: var(--text-light);
            font-style: italic;
        }

        pre {
            background: var(--light-bg);
            padding: 20px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 20px 0;
        }

        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 5px;
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            margin-top: 40px;
            transition: gap 0.3s ease;
        }

        .back-link:hover {
            gap: 10px;
        }

        .post-actions-bar {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            padding: 20px;
            border-radius: 50px;
            box-shadow: var(--shadow-lg);
            display: flex;
            justify-content: center;
            gap: 30px;
            margin: 40px 0;
            border: 1px solid var(--border-color);
        }

        .post-actions-bar .like-btn,
        .post-actions-bar .comment-btn,
        .post-actions-bar .share-btn {
            background: none;
            border: none;
            color: var(--text-dark);
            cursor: pointer;
            font-size: 1rem;
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 10px 20px;
            border-radius: 25px;
            transition: all 0.3s ease;
            font-weight: 500;
        }

        .post-actions-bar .like-btn:hover,
        .post-actions-bar .comment-btn:hover,
        .post-actions-bar .share-btn:hover {
            background: var(--light-bg);
            color: var(--accent-color);
        }

        .post-actions-bar .like-btn.liked {
            color: var(--danger-color);
        }

        .post-comment-section {
            margin-top: 30px;
            padding-top: 30px;
            border-top: 1px solid var(--border-color);
        }
    </style>
</head>
<body>
<header>
    <div class="container">
        <nav>
            <a href="../index.html" class="nav-logo">Haitao Wang</a>
            <div class="nav-links">
                <a href="../index.html">Home</a>
                <a href="../blog.html">Blog</a>
                <a href="../projects.html">Projects</a>
                <a href="../resources.html">Resources</a>
                <a href="../forum.html">Forum</a>
            </div>
        </nav>
    </div>
</header>

<div class="container">
    <article class="blog-post-content" data-post-id="2">
        <h1>Chapter 2: Exact Solution Methods - Value Iteration and Policy Iteration</h1>
        <p style="text-align: center; color: var(--text-light); font-style: italic; margin-bottom: 30px;">Fundamentals and Foundations</p>

        <div class="blog-meta">
            <span><i class="far fa-calendar"></i> July 17, 2025</span>
            <span><i class="far fa-clock"></i> 18 min read</span>
            <span class="tag">value iteration</span>
            <span class="tag">policy iteration</span>
            <span class="tag">MDP</span>
        </div>

        <h2>1. Introduction</h2>

        <p>
            Having established the mathematical foundations of Markov Decision Processes in Chapter 1, we
            now turn our attention to the classical methods for solving MDPs when the environment dynamics
            are fully known, and the state-action space is finite and relatively small. These analytical solutions
            guarantee convergence to the optimal policy given perfect knowledge of the environment's transition
            dynamics and reward structure. It is important to learn and understand these foundational RL
            algorithms because of two reasons:
        </p>

        <ol>
            <li><strong>Theoretical Foundation:</strong> They provide the mathematical basis for understanding optimality in sequential decision-making</li>
            <li><strong>Algorithmic Intuition:</strong> Approximate methods like Deep Q-learning and policy gradient
                methods which will be introduced in later chapters can be understood as extensions of these
                classical approaches.</li>
        </ol>

        <h2>2. The Bellman Equations</h2>

        <p>
            Before diving into the algorithms, we must understand the Bellman equations, which form the
            mathematical foundation of both value iteration and policy iteration. These equations capture the
            recursive nature of decision-making process and is a critical aspect of value iteration and policy
            iteration algorithms.
        </p>

        <h3>2.1 The Bellman Optimality Equations</h3>

        <p>
            For any MDP, the optimal state value function $V^*(s)$ represents the maximum expected return
            achievable from starting state $s$. This function satisfies the Bellman optimality equation:
        </p>

        <div class="math-block">
            $$V^*(s) = \max_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^*(s')]$$
        </div>

        <p>
            More acute readers may have noticed that the recursive structure in Bellman equation allows us
            to break down complex sequential decision problems into simpler subproblems, just like dynamic
            programming. And we will see how it's used in practice in the following sections.
        </p>

        <h3>2.2 Policy Evaluation and the Bellman Equation for a Given Policy</h3>

        <p>
            Bellman optimality equation gives us a greedy solution by always taking the action that will lead to
            the maximum expected return. However, if we want to parameterise and learn the policy function
            explicitly, we will need to represent the value function conditioned on a fixed policy $\pi$ starting from
            state $s$ as $V^\pi(s)$:
        </p>

        <div class="math-block">
            $$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^\pi(s')]$$
        </div>

        <p>
            This linear system of equations can be solved exactly using matrix operations or iteratively
            through repeated application of the Bellman operator.
        </p>

        <h2>3. Value Iteration Algorithm</h2>

        <p>
            As mentioned before, in value iteration, we follow a policy by taking greedy action based on learned
            value function over the next states. The optimal value of state $s$ is defined formally as below:
        </p>

        <div class="math-block">
            $$V^*(s) = \max_a \mathbb{E}\left[\sum_{t=0}^H \gamma^t R(s_t, a_t, s_{t+1}) \mid \pi, s_0 = s\right]$$
        </div>

        <p>and our optimal policy upon convergence is defined as:</p>

        <div class="math-block">
            $$\pi^*(s) = \arg\max_{a \in A} \sum_{s'} T(s, a, s')[R(s, a, s') + \gamma V^*(s')]$$
        </div>

        <p>
            When translated into plain English: the value of state $s$ is the expected sum of cumulative
            rewards over horizon $H$ under policy $\pi$, starting from state $s$. Value iteration directly applies the
            Bellman optimality equation as an iterative update rule. The algorithm maintains estimates of the
            optimal value function and improves these estimates at each iteration until convergence.
        </p>

        <h3>3.1 The Algorithm</h3>

        <div class="algorithm-box">
            <p><strong>The value iteration algorithm proceeds as follows:</strong></p>
            <ol>
                <li><strong>Initialization:</strong> Set $V_0(s) = 0$ for all states $s$ (or any arbitrary values)</li>
                <li><strong>Iteration:</strong> For $k = 0, 1, 2, \ldots$ until convergence:
                    <div class="math-block">
                        $$V_{k+1}(s) = \max_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V_k(s')]$$
                    </div>
                </li>
                <li><strong>Optimal policy upon convergence:</strong>
                    <div class="math-block">
                        $$\pi^*(s) = \arg\max_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^*(s')]$$
                    </div>
                </li>
            </ol>
        </div>

        <figure class="blog-figure">
            <img src="../src/chapter2/example.png" alt="Gridworld example for value iteration">
            <figcaption>Example gridworld environment with robot, obstacles, and goal state. In the environment, the robot's action has 80%
                chance of succeeding and 0.9 discount factor  </figcaption>
        </figure>

        <div class="iteration-images">
            <div class="iteration-image">
                <img src="../src/chapter2/ite1.png" alt="Value iteration at k=0">
                <p>Initial values (k=0): All states initialized to 0</p>
            </div>
            <div class="iteration-image">
                <img src="../src/chapter2/ite2.png" alt="Value iteration at k=2">
                <p>After 2 iterations: Values propagating from terminal states</p>
            </div>
            <div class="iteration-image">
                <img src="../src/chapter2/ite3.png" alt="Value iteration at k=3">
                <p>After 3 iterations: Values continuing to propagate</p>
            </div>
            <div class="iteration-image">
                <img src="../src/chapter2/ite4.png" alt="Value iteration at k=4">
                <p>After 4 iterations: Further refinement of values</p>
            </div>
        </div>

        <figure class="blog-figure">
            <img src="../src/chapter2/ite12.png" alt="Value iteration at k=12">
            <figcaption>After 12 iterations: Values approaching convergence</figcaption>
        </figure>

        <figure class="blog-figure">
            <img src="../src/chapter2/ite100.png" alt="Value iteration at k=100">
            <figcaption>After 100 iterations: Converged optimal values</figcaption>
        </figure>

        <h3>3.2 Convergence Analysis</h3>

        <p>
            So far we have assumed that value iteration algorithms will converge to an optimal and unique policy
            without explaining why, in this section we will go through a simple intuitive proof of convergence
            and also outlining a more formal proof.
        </p>

        <p>
            <strong>Intuitive Convergence Proof:</strong> Consider what happens if we continue applying the Bellman
            update equation after convergence at step $H$. For any state $s_{new}$, the future discounted rewards
            from step $H + 1, H + 2, \ldots$ onward are:
        </p>

        <div class="math-block">
            $$\begin{align}
            \lim_{H \to \infty} R(s_{new}) + \gamma^{H+1}R(s_{new}) + \gamma^{H+2}R(s_{new}) + \ldots &\\
            &\leq \lim_{H \to \infty} \gamma^{H+1}R_{max} + \gamma^{H+2}R_{max} + \ldots \\
            &= \lim_{H \to \infty} \frac{\gamma^{H+1}}{1 - \gamma} R_{max} \\
            &= 0
            \end{align}$$
        </div>

        <p>
            This shows that the contribution of future rewards becomes negligible as we go further into the
            future, ensuring that the value function stabilizes. More formally, we can use contraction mapping
            theorem to prove the convergence.
        </p>

        <p>
            <strong>Contraction Mapping Theorem:</strong> A mapping $T : \mathbb{R}^n \to \mathbb{R}^n$
            is a contraction if there exists a constant $0 \leq \lambda < 1$ such that for all $x, y \in \mathbb{R}^n$:
        </p>

        <div class="math-block">
            $$||T(x) - T(y)|| \leq \lambda||x - y||$$
        </div>

        <p>Where the Bellman optimality operator $T$ defined by:</p>

        <div class="math-block">
            $$T(s) = \max_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V(s')]$$
        </div>

        <p>
            We can show that for any MDP with discount factor $\gamma < 1$, value iteration converges to the
            optimal value function $V^*$. Moreover, the convergence is geometric with rate $\gamma$:
        </p>

        <div class="math-block">
            $$||V_k - V^*||_\infty \leq \gamma^k ||V_0 - V^*||_\infty$$
        </div>

        <p>
            This result tells us that value iteration converges exponentially fast, with the rate determined
            by the discount factor. Additionally, we also showed that smaller discount factor leads to faster
            convergence. When implementing value iteration, several practical considerations arise:
        </p>

        <p>
            <strong>Stopping Criteria:</strong> In practice, we don't iterate until exact convergence but stop when the
            value function changes by less than a small threshold $\epsilon$:
        </p>

        <div class="math-block">
            $$\max_s |V_{k+1}(s) - V_k(s)| < \epsilon$$
        </div>

        <h2>4. Policy Iteration Algorithm</h2>

        <p>
            Policy iteration takes a different approach, instead of always taking greedy action based on value
            function, we attempt to learn and iteratively improve the policy $\pi$ directly, by alternating between
            policy evaluation (computing the value function under current policy) and policy improvement
            (updating the policy based on the current value function).
        </p>

        <h3>4.1 The Algorithm</h3>

        <div class="algorithm-box">
            <p><strong>Policy iteration consists of two main steps repeated until convergence:</strong></p>
            <ol>
                <li><strong>Policy Evaluation:</strong> Given current policy $\pi_k$, solve for its value function:
                    <div class="math-block">
                        $$V^{\pi_k}(s) = \sum_a \pi_k(a|s) \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^{\pi_k}(s')]$$
                    </div>
                </li>
                <li><strong>Policy Improvement:</strong> Update the policy greedily with respect to the current value function:
                    <div class="math-block">
                        $$\pi_{k+1}(s) = \arg\max_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^{\pi_k}(s')]$$
                    </div>
                </li>
                <li><strong>Convergence Condition:</strong> If $\pi_{k+1} = \pi_k$, then the algorithm has converged to the optimal
                    policy.</li>
            </ol>
        </div>

        <h3>4.2 Policy Evaluation in Detail</h3>

        <p>
            The policy evaluation step involves solving a system of linear equations. For a policy $\pi$, we need
            to solve:
        </p>

        <div class="math-block">
            $$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^\pi(s')]$$
        </div>

        <p>This can be approached in several ways:</p>

        <p>
            <strong>Direct Solution:</strong> Rearrange as a linear system $(I - \gamma P_\pi)V^\pi = R_\pi$ and solve using matrix
            inversion. This is exact but computationally expensive for large state spaces.
        </p>

        <p><strong>Iterative Policy Evaluation:</strong> Apply the Bellman operator repeatedly:</p>

        <div class="math-block">
            $$V^\pi_{k+1}(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^\pi_k(s')]$$
        </div>

        <h3>4.3 Convergence Analysis</h3>

        <p>
            Similar to value iteration, policy iteration will also converge to the same optimal policy, the intuition
            on how to prove its convergence is shown below:
        </p>

        <ol>
            <li>
                <strong>Policy iteration is guaranteed to converge</strong> due to the policy improvement theorem,
                given that the state-action space is finite (there are only finitely many deterministic policies),
                and each iteration either improves the policy or finds that it's already optimal. This leads to
                convergence in at most $|A|^{|S|}$ iterations.
            </li>
            <li>
                <strong>Optimality at convergence:</strong> At convergence, $\pi_{new} = \pi_k(s)$, hence $V_s, V^{\pi_k}(s) =
                \max_a \sum_{s'} P(s, a, s')[R(s, a, s') + \gamma V^{\pi_k}(s')]$ which satisfies the Bellman equation, therefore $V^{\pi_k}$
                is equal to the optimal value function $V^*$.
            </li>
        </ol>

        <h2>5. Conclusion</h2>

        <p>
            In this chapter, we have covered Bellman Optimality Equation, followed by a deep dive into value
            iteration and policy iteration algorithm and proof of convergence and optimality for both algorithms. While these two basic algorithms assume discrete state and action spaces, we can leverage
            neural networks to approximate policy and value functions. This is called Deep Reinforcement
            Learning (DRL) and we will learn more about DRL in the upcoming chapters starting from Deep
            Q-learning which on a conceptual level works very similarly to value iteration.
        </p>

        <p>
            Understanding these exact methods provides the foundation for appreciating both the power
            and limitations of approximate methods in reinforcement learning. When environment dynamics
            are known or learnable, these classical approaches often provide robust, interpretable solutions that
            serve as excellent baselines for more complex algorithms.
        </p>

        <!-- Post actions bar -->
        <div class="post-actions-bar">
            <button class="like-btn" onclick="toggleLike(2)">
                <i class="far fa-heart"></i> <span class="like-count">0</span>
            </button>
            <button class="comment-btn" onclick="toggleComments(2)">
                <i class="far fa-comment"></i> <span class="comment-count">0</span>
            </button>
            <span class="view-count"><i class="far fa-eye"></i> 0</span>
            <button class="share-btn" onclick="sharePost()">
                <i class="fas fa-share"></i> Share
            </button>
        </div>

        <!-- Comments Section -->
        <div class="comments-section post-comment-section" id="comments-3">
            <h3>Comments</h3>
            <div class="comment-form">
                <input type="text" placeholder="Your name" class="comment-name" id="comment-name-3">
                <textarea placeholder="Share your thoughts..." class="comment-text" id="comment-text-3"></textarea>
                <button class="btn-primary" onclick="addComment(2)">Post Comment</button>
            </div>
            <div class="comments-list" id="comments-list-3">
                <!-- Comments will be loaded here -->
            </div>
        </div>

        <a href="../blog.html" class="back-link">
            <i class="fas fa-arrow-left"></i> Back to Blog
        </a>
    </article>
</div>

<footer>
    <div class="container">
        <p>&copy; 2025 Haitao Wang. All rights reserved.</p>
    </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/contrib/auto-render.min.js"></script>
<script src="../js/main.js"></script>
<script>
    // Initialize KaTeX rendering
    renderMathInElement(document.body, {
        delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
        ]
    });
</script>
</body>
</html>