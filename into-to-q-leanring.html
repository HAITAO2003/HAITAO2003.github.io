<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Q-Learning - Haitao Wang</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        .blog-post {
            max-width: 800px;
            margin: 0 auto;
            padding: 60px 20px;
        }

        .post-header {
            text-align: center;
            margin-bottom: 50px;
        }

        .post-title {
            font-size: 2.5rem;
            font-weight: 800;
            margin-bottom: 20px;
            color: var(--text-dark);
        }

        .post-meta-info {
            display: flex;
            justify-content: center;
            gap: 30px;
            color: var(--text-light);
            margin-bottom: 30px;
        }

        .post-content {
            font-size: 1.1rem;
            line-height: 1.8;
            color: var(--text-dark);
        }

        .post-content h2 {
            margin-top: 40px;
            margin-bottom: 20px;
            color: var(--primary-color);
        }

        .post-content h3 {
            margin-top: 30px;
            margin-bottom: 15px;
        }

        .post-content p {
            margin-bottom: 20px;
        }

        .post-content pre {
            background: #f4f4f4;
            padding: 20px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 20px 0;
        }

        .post-content code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
        }

        .post-content pre code {
            background: none;
            padding: 0;
        }

        .post-actions-bar {
            position: sticky;
            bottom: 20px;
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            padding: 20px;
            border-radius: 50px;
            box-shadow: var(--shadow-lg);
            display: flex;
            justify-content: center;
            gap: 30px;
            margin: 40px 0;
        }

        .post-comment-section {
            margin-top: 60px;
            padding-top: 40px;
            border-top: 2px solid var(--border-color);
        }
    </style>
</head>
<body>
<header>
    <div class="container">
        <nav>
            <a href="index.html" class="nav-logo">Haitao Wang</a>
            <div class="nav-links">
                <a href="index.html">Home</a>
                <a href="blog.html">Blog</a>
                <a href="projects.html">Projects</a>
                <a href="forum.html">Forum</a>
            </div>
        </nav>
    </div>
</header>

<article class="blog-post" data-post-id="1">
    <div class="post-header">
        <h1 class="post-title">Introduction to Q-Learning</h1>
        <div class="post-meta-info">
            <span><i class="far fa-calendar"></i> January 15, 2024</span>
            <span><i class="far fa-clock"></i> 8 min read</span>
            <span><i class="far fa-eye"></i> <span class="view-count">1.2k</span> views</span>
        </div>
        <div class="blog-tags">
            <span class="tag">q-learning</span>
            <span class="tag">basics</span>
            <span class="tag">reinforcement-learning</span>
        </div>
    </div>

    <div class="post-content">
        <p>
            Q-Learning is one of the foundational algorithms in reinforcement learning, and understanding it is crucial
            for anyone looking to dive deeper into the field. In this post, we'll explore the mathematical foundations
            of Q-Learning, implement it from scratch, and see it in action on a simple grid world environment.
        </p>

        <h2>What is Q-Learning?</h2>
        <p>
            Q-Learning is a model-free, off-policy reinforcement learning algorithm that learns the value of actions
            in particular states. The "Q" in Q-Learning stands for quality, representing the quality of taking a
            certain action in a given state.
        </p>

        <p>
            The core idea is to learn a function Q(s, a) that tells us the expected future reward of taking action
            <em>a</em> in state <em>s</em>. Once we have a good approximation of this Q-function, we can derive an
            optimal policy by simply choosing the action with the highest Q-value in each state.
        </p>

        <h2>The Bellman Equation</h2>
        <p>
            At the heart of Q-Learning lies the Bellman equation, which provides a recursive relationship for the
            optimal Q-values:
        </p>

        <p style="text-align: center;">
            $$Q^*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} Q^*(s', a') | S_t = s, A_t = a]$$
        </p>

        <p>
            Where:
        </p>
        <ul>
            <li>$Q^*(s, a)$ is the optimal action-value function</li>
            <li>$R_{t+1}$ is the immediate reward</li>
            <li>$\gamma$ is the discount factor (0 ≤ γ < 1)</li>
            <li>$s'$ is the next state</li>
            <li>$\max_{a'} Q^*(s', a')$ is the maximum Q-value for the next state</li>
        </ul>

        <h2>The Q-Learning Algorithm</h2>
        <p>
            Q-Learning uses temporal difference (TD) learning to iteratively update Q-values based on experience.
            The update rule is:
        </p>

        <p style="text-align: center;">
            $$Q(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$
        </p>

        <p>
            Where α is the learning rate that controls how much we update our estimates.
        </p>

        <h3>Implementation</h3>
        <p>Let's implement a simple Q-Learning agent for a grid world environment:</p>

        <pre><code class="language-python">import numpy as np
import random

class QLearningAgent:
    def __init__(self, n_states, n_actions, learning_rate=0.1,
                 discount_factor=0.9, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon

        # Initialize Q-table with zeros
        self.q_table = np.zeros((n_states, n_actions))

    def choose_action(self, state):
        # Epsilon-greedy action selection
        if random.random() < self.epsilon:
            return random.randint(0, self.n_actions - 1)
        else:
            return np.argmax(self.q_table[state])

    def update(self, state, action, reward, next_state):
        # Q-Learning update rule
        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])
        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state, action] = new_q</code></pre>

        <h2>Exploration vs Exploitation</h2>
        <p>
            One of the key challenges in reinforcement learning is balancing exploration (trying new actions to
            discover their rewards) with exploitation (using current knowledge to maximize rewards). Q-Learning
            typically uses ε-greedy exploration, where with probability ε we take a random action, and otherwise
            we take the greedy action.
        </p>

        <h2>Convergence Guarantees</h2>
        <p>
            Under certain conditions, Q-Learning is guaranteed to converge to the optimal Q-function:
        </p>
        <ul>
            <li>All state-action pairs must be visited infinitely often</li>
            <li>The learning rate must decay appropriately</li>
            <li>The environment must be stationary</li>
        </ul>

        <h2>Limitations and Extensions</h2>
        <p>
            While Q-Learning is powerful, it has limitations:
        </p>
        <ul>
            <li><strong>Overestimation bias:</strong> Q-Learning tends to overestimate Q-values due to the max operator</li>
            <li><strong>Scalability:</strong> Tabular Q-Learning doesn't scale to large or continuous state spaces</li>
            <li><strong>Sample efficiency:</strong> Can require many interactions with the environment</li>
        </ul>

        <p>
            These limitations have led to various extensions like Double Q-Learning (addressing overestimation),
            Deep Q-Networks (handling large state spaces), and more sophisticated exploration strategies.
        </p>

        <h2>Conclusion</h2>
        <p>
            Q-Learning remains one of the most important algorithms in reinforcement learning. Its simplicity and
            theoretical guarantees make it an excellent starting point for understanding RL. In future posts, we'll
            explore how neural networks can be combined with Q-Learning to create Deep Q-Networks (DQN), enabling
            us to tackle much more complex problems.
        </p>
    </div>

    <div class="post-actions-bar">
        <button class="like-btn" onclick="toggleLike(1)">
            <i class="far fa-heart"></i> <span class="like-count">42</span> Likes
        </button>
        <button class="comment-btn" onclick="toggleComments(1)">
            <i class="far fa-comment"></i> <span class="comment-count">12</span> Comments
        </button>
        <button class="share-btn" onclick="sharePost()">
            <i class="fas fa-share"></i> Share
        </button>
    </div>

    <div class="post-comment-section">
        <h2>Comments</h2>

        <div class="comment-form">
            <input type="text" placeholder="Your name" class="comment-name" id="comment-name-1">
            <textarea placeholder="Share your thoughts about this post..." class="comment-text" id="comment-text-1"></textarea>
            <button class="btn-primary" onclick="addComment(1)">Post Comment</button>
        </div>

        <div class="comments-list" id="comments-list-1">
            <div class="comment">
                <div class="comment-header">
                    <strong>Sarah Chen</strong>
                    <span class="comment-date">2 days ago</span>
                </div>
                <p>Great introduction! The step-by-step breakdown of the Bellman equation really helped me understand the concept.</p>
            </div>
            <div class="comment">
                <div class="comment-header">
                    <strong>Alex Kumar</strong>
                    <span class="comment-date">1 week ago</span>
                </div>
                <p>Would love to see a follow-up on Double Q-Learning to address the overestimation bias!</p>
            </div>
        </div>
    </div>
</article>

<footer>
    <div class="container">
        <p>&copy; 2024 Haitao Wang. Built with passion for RL.</p>
    </div>
</footer>

<script src="js/main.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
    renderMathInElement(document.body, {
        delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
        ]
    });

    function sharePost() {
        if (navigator.share) {
            navigator.share({
                title: 'Introduction to Q-Learning',
                text: 'Check out this great introduction to Q-Learning!',
                url: window.location.href
            });
        } else {
            // Fallback - copy to clipboard
            navigator.clipboard.writeText(window.location.href);
            alert('Link copied to clipboard!');
        }
    }
</script>
</body>
</html>